---
title: "Kaggle competition Prudential Life Insurance Assessment"
output: html_notebook
---

This notebook defines our solution for the Kaggle competition "Prudential Life Insurance Assessment".

```{r}
library(mlr3)
library(mlr3verse)
library(mlr3learners)
library(mlr3pipelines)
library(Metrics)
```

## Load the data

```{r}
set.seed(123)

data <- read.csv("data\\train.csv")
data_submission <- read.csv("data\\test.csv")

# create a mlr3 task
task_regr <- as_task_regr(data, target = "Response")

# split the data into training and testing sets
split <- partition(task_regr, ratio = 0.8)
```

## Preprocess the data using ml3 pipelines

The data contains many features and requires intensive preprocessing to successfully train a model.

```{r}	
# for Medical_Keyword_1-48 are all binary (0-1): construct new feature sum keywords
features_medical_keywords <- paste0("Medical_Keyword_", 1:48)

sum_columns <- function(data, columns) {
    if (length(columns) == 0) {
        stop("No columns to sum.")
    }
    data = as.data.table(data)
    # Ensure the result is a data.table with a column name
    data.table(sum_medical_keywords = rowSums(data[, ..columns]))
}

sum_medical_keywords <- sum_columns(task_regr$data(), features_medical_keywords)
task_regr$cbind(sum_medical_keywords)

# do the same for the submission data
sum_medical_keywords_submission <- sum_columns(data_submission, features_medical_keywords)
data_submission <- cbind(data_submission, sum_medical_keywords_submission)
```

```{r}
data_train <- task_regr$data(rows = split$train)

task_train <- task_regr$clone(deep = TRUE)$filter(split$train)
task_test <- task_regr$clone(deep = TRUE)$filter(split$test)
```

```{r}
# get a list of features with more than 30% missing values
threshold <- 0.3
missing_features_rmv <- colnames(data_train)[colMeans(is.na(data_train)) > threshold]
selected_features <- setdiff(colnames(data_train), append(missing_features_rmv, "Id"))
```

Create a pipeline to preprocess the data.

```{r}
features_impute_mean <- c("Employment_Info_1", "Employment_Info_4", "Employment_Info_6")
features_imput_mode <- c("Medical_History_1")
po_select <- po("select", selector = selector_name(selected_features))
po_impute <- po(
    "imputemean",
    affect_columns = selector_name(features_impute_mean)
) %>>% po(
    "imputemode",
    affect_columns = selector_name(features_imput_mode)
)
```

```{r}	
# ordinal encoding
features_encode_ordinal <- c("Product_Info_2")
ordinal_encode <- function(x) {
    as.integer(as.factor(x))
}
po_ordinal <- po("colapply",
    id = "ordinal_encode",
    applicator = ordinal_encode,
    affect_columns = selector_name(features_encode_ordinal)
)

# binary encoding
features_encode_binary <- c("Product_Info_1", "Product_Info_5", "Product_Info_6", "Employment_Info_3", "Employment_Info_5", "InsuredInfo_2", "InsuredInfo_4", "InsuredInfo_5", "InsuredInfo_6", "InsuredInfo_7", "Insurance_History_1")
binary_encode <- function(x) {
    x <- as.factor(x)
    levels_x <- levels(x)
    if (length(levels_x) != 2) stop("The variable must have exactly two levels")
    encoded <- as.integer(x) - 1
    if (!all(encoded %in% c(0, 1))) stop("Resulting values are not all 0 or 1")
    encoded
}
po_binary <- po("colapply",
    id = "binary_encode",
    applicator = binary_encode,
    affect_columns = selector_name(features_encode_binary)
)

# one-hot encoding
# features_encode_onehot <- 
# po_onehot <- po("encode",
#     affect_columns = selector_name(features_encode_onehot, assert_present = TRUE),
#     method = "one-hot"
# )

# impact encoding
features_encode_impact <- c("Product_Info_7", "InsuredInfo_1", "Insurance_History_2", "Insurance_History_3", "Insurance_History_4", "Insurance_History_7", "Insurance_History_8", "Insurance_History_9", "Family_Hist_1")
convert_to_factors <- function(x) {
    as.factor(x)
}
po_impact <- po("colapply",
    id = "convert_to_factors",
    applicator = convert_to_factors,
    affect_columns = selector_name(features_encode_impact)
) %>>% po("encodeimpact", affect_columns = selector_name(features_encode_impact))
```

```{r}
# perform a pca on the medical keywords 1-48, selecting the first 7 pcs
features_pca <- paste0("Medical_Keyword_", 1:48)
po_pca <- po("pca", id="pca_medical_keyword", affect_columns = selector_name(features_pca), rank. = 7)

# remove the original medical keywords
po_remove_medical_keywords <- po("select", id = "remove_medical_keywords", selector = selector_invert(selector_name(features_pca)))

# rename the pca columns: po("renamecolumns", param_vals = list(renaming = c("Petal.Length" = "PL")))
name_mapping <- setNames(paste0("Medical_Keyword_PC", 1:7), paste0("PC", 1:7))
po_rename_pca <- po("renamecolumns", id="rename_keyword_pcs", renaming = name_mapping)

po_medical_keywords <- po_pca %>>% po_remove_medical_keywords %>>% po_rename_pca
```

```{r}
# for medical history 1-41, impact encode columns 3-41 and then perform pca on all 41
features_medical_history <- paste0("Medical_History_", 3:41)
po_medical_history_impact <- po("encodeimpact", id="impact_enc_medical_history", affect_columns = selector_name(features_medical_history))
po_medical_history_scale <- po("scalerange", id="scale_medical_history", affect_columns = selector_name(paste0("Medical_History_", 1:41)))
po_medical_history_pca <- po("pca", id="pca_medical_history", affect_columns = selector_name(paste0("Medical_History_", 1:41)), rank. = 10)
po_remove_medical_history <- po("select", id = "remove_medical_history", selector = selector_invert(selector_name(paste0("Medical_History_", 1:41))))
name_mapping <- setNames(paste0("Medical_History_PC", 1:10), paste0("PC", 1:10))
po_rename_history_pca <- po("renamecolumns", id="rename_history_pcs", renaming = name_mapping)

po_medical_history <- po_medical_history_impact %>>% po_medical_history_scale %>>% po_medical_history_pca %>>% po_remove_medical_history %>>% po_rename_history_pca
```

```{r}
po_scale <- po("scalerange")
```


```{r}
pipeline <- po_select %>>% po_impute %>>% po_binary %>>% po_ordinal %>>% po_impact %>>% po_medical_keywords %>>% po_medical_history %>>% po_scale
```

```{r}
# check the result of the pipeline
result <- pipeline$train(task_train)

data_res <- result[[1]]$data()
# print columns of data_res
head(data_res)
```


```{r}
level0 <- pipeline %>>% po("learner_cv", learner = lrn("regr.ranger"), resampling.method = "cv", resampling.folds = 3)

# level1 <- lrn("classif.rpart", id="rpart_out")
# learner <- as_learner(level0 %>>% level1)

# learner$train(task_regr, row_ids = split$train)

# source("scripts/qwk_measure.R")
# measure <- MeasureRegrQuadraticWeightedKappa$new()

# learner$predict(task_regr, row_ids = split$test)$score(measure)
```

```{r}	
level0_train_output = level0$train(task_regr)
level0_train_output
```


```{r}	
level0_out_classif = as_task_classif(level0_train_output[[1]]$data(), target = "Response")
level1 <- lrn("classif.rpart", id="rpart_out")
level1$train(level0_out_classif)
```

```{r}	
source("scripts/qwk_measure.R")
measure <- MeasureClassifQuadraticWeightedKappa$new()

task_test = as_task_regr(task_regr$data(rows = split$test), target = "Response")

level0_test_out = level0$predict(task_test)
level0_test_out_classif = as_task_classif(level0_test_out[[1]]$data(), target = "Response")
level1$predict(level0_test_out_classif)$score(measure)
```

```{r}
# add a column response to the submission data random between 1 and 8
data_submission$Response = sample(1:8, nrow(data_submission), replace=TRUE)
level0_submission_output = level0$predict(as_task_regr(data_submission, target="Response"))
level0_submission_out_classif = as_task_classif(level0_submission_output[[1]]$data(), target = "Response")
```

```{r}	
current_datetime <- format(Sys.time(), "%Y%m%d%H%M")
filename <- paste0("submissions/submission_", "stacked", current_datetime, ".csv")

preds <- level1$predict_newdata(newdata = level0_submission_out_classif$data())
results <- data.frame(Id = data_submission$Id, Response = as.integer(preds$response))

write.csv(results, filename, row.names = FALSE)
```	

```{r}
source("scripts/utils.R")
create_learner_submission(level1, level0_submission_out_classif, name="ranger_pipe")
```

```{r}
# ---- Stacking ----
# learner_regr <- lrn("regr.xgboost")
# learner_classif <- lrn("classif.xgboost", predict_type = "prob")
# learner_tree <- lrn("regr.rpart")

# # Define the preprocessing pipeline
# preprocessing_pipeline <- pipeline

# # Define the regression branch
# regression_branch <- po("copy") %>>%
#   preprocessing_pipeline %>>%
#   po("learner_cv", learner = learner_regr, resampling.method = "cv", resampling.folds = 3)

# # Define the classification branch
# classification_branch <- po("copy") %>>%
#   po_regr_to_classif %>>%
#   preprocessing_pipeline %>>%
#   po("learner_cv", learner = learner_classif, resampling.method = "cv", resampling.folds = 3)

# # Combine the branches
# combined <- gunion(list(regression_branch, classification_branch)) %>>%
#   po("featureunion")

# # Define the final pipeline
# final_pipeline <- preprocessing_pipeline %>>%
#   combined %>>%
#   po("learner", learner = learner_tree)

# # ---- Training ----

# # Train the final pipeline
# result <- final_pipeline$train(task_train)

# # Print the result
# print(result)


```	

